{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75f41de7-34b3-4fec-bf4c-295f7e8b45ff",
   "metadata": {},
   "source": [
    "# MNIST binary (0, 1, 2, 3, 4 vs 5, 6, 7, 8, 9) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8546fab-ad68-4215-bfc4-2e954ae8785d",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1421f61d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, sys, time, math, numpy, torch, torchvision, matplotlib.pyplot\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))  # Allow repository modules to be imported\n",
    "\n",
    "from utils.optimization import initialize\n",
    "\n",
    "data_setting = {\n",
    "    'seed': 1234,\n",
    "    'sample_size': 60000, \n",
    "    'batch_size': 50,\n",
    "    'input_dimension': 28 * 28\n",
    "}\n",
    "\n",
    "model_setting = {\n",
    "    'epochs': 10000,\n",
    "    'learning_rate': 0.01,\n",
    "    'batch_size': data_setting['batch_size'],\n",
    "    'initial_hidden_units': 2,\n",
    "    'bias': True,\n",
    "}\n",
    "\n",
    "experiment = {\n",
    "    **data_setting,\n",
    "    **model_setting,\n",
    "    'convergence_epsilon': 0.1,\n",
    "    'train': 'cross entropy',\n",
    "    'test': 'cross entropy',\n",
    "    'train_time': 'seconds',\n",
    "    'models_runs': []\n",
    "}\n",
    "\n",
    "initialize(experiment['seed'])\n",
    "\n",
    "transform_train = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomCrop(28, padding=4),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    torchvision.transforms.Lambda(torch.flatten)\n",
    "])\n",
    "\n",
    "transform_test = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    torchvision.transforms.Lambda(torch.flatten)\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./mnist/', train=True, download=True, transform=transform_train)\n",
    "train_data = torch.utils.data.DataLoader(train_dataset, batch_size=data_setting['batch_size'], shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./mnist/', train=False, download=True, transform=transform_test)\n",
    "test_data = torch.utils.data.DataLoader(test_dataset, batch_size=data_setting['batch_size'], shuffle=False)\n",
    "\n",
    "\n",
    "_, (inputs, labels) = next(enumerate(test_data))\n",
    "\n",
    "fig, ax = matplotlib.pyplot.subplots(figsize=(5, 5))\n",
    "ax.imshow(inputs[0].reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af03851b-250c-4b33-95df-681aec122c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(seed=123):\n",
    "    numpy.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    device = (\n",
    "        \"cuda\"\n",
    "        if torch.cuda.is_available()\n",
    "        else \"mps\"\n",
    "        if torch.backends.mps.is_available()\n",
    "        else \"cpu\"\n",
    "    )\n",
    "    return device\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, device, verbose=False, callback=None, retain_graph=False):\n",
    "    num_batches = len(dataloader)\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    train_loss = 0\n",
    "    for X, y in dataloader:\n",
    "        y = (y < 5).float().unsqueeze(1)\n",
    "        X, y = X.to(torch.float32).to(device), y.to(torch.float32).to(device)\n",
    "        loss = loss_fn(model(X), y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward(retain_graph=retain_graph)\n",
    "        if callback: callback(model, loss) # Extract information about the gradients before these are applied and zero out.\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    train_loss /= num_batches\n",
    "    if verbose: print(f\"Train Avg loss: {train_loss:>8f}\")\n",
    "    return train_loss\n",
    "\n",
    "def test(dataloader, model, loss_fn, device, verbose=False, calculate_gradients=False, callback=None, retain_graph=False):\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for X, y in dataloader:\n",
    "        y = (y < 5).float().unsqueeze(1)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        loss = loss_fn(model(X), y)\n",
    "        test_loss += loss.item()\n",
    "        if calculate_gradients: loss.backward(retain_graph=retain_graph)\n",
    "        if callback: callback(model, loss)\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    if verbose: print(f\"Test Avg loss: {test_loss:>8f}\\n\")    \n",
    "    return test_loss\n",
    "\n",
    "def Accuracy(pred, label): \n",
    "    \"https://stackoverflow.com/questions/51503851/calculate-the-accuracy-every-epoch-in-pytorch/63271002#63271002\"\n",
    "    if len(pred.shape) == 1 or pred.size(1) == 1:\n",
    "        # Binary classification\n",
    "        return ((torch.sign(pred) + 1.) * 0.5  == label).sum() / pred.size(0)\n",
    "    \n",
    "    else:\n",
    "        # Multiclass classification\n",
    "        return (torch.argmax(pred, dim=1) == label).sum() / pred.size(0)\n",
    "\n",
    "class GrowingNeuralNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dimension:int, learning_rate:float=0., margin:float=0., noise_rate:float=0., input_domain_radious:float=1.,\n",
    "                 initial_hidden_units:int=1, initial_depth:int=1, bias:bool=True, *args, **kwargs):\n",
    "        super(GrowingNeuralNet, self).__init__()\n",
    "\n",
    "        self.device = 'cpu'\n",
    "        self.input_dimension = input_dimension\n",
    "        self.initial_hidden_units = initial_hidden_units\n",
    "        self.learning_rate = learning_rate\n",
    "        self.margin = margin\n",
    "        self.noise_rate = noise_rate\n",
    "        self.input_domain_radious = input_domain_radious\n",
    "        self.bias = bias\n",
    "        self.layers = [torch.nn.Linear(input_dimension if layer_index == 0 else initial_hidden_units, initial_hidden_units, bias=bias) \n",
    "                       for layer_index in range(initial_depth)]\n",
    "        self.residual_norm = numpy.nan\n",
    "        self.gradients_average_norm = [[numpy.nan,] * initial_hidden_units, ] * initial_depth\n",
    "        self.average_gradient_norm = [[numpy.nan,] * initial_hidden_units, ] * initial_depth\n",
    "        self.residual_projection_norm = [[numpy.nan,] * initial_hidden_units, ] * initial_depth\n",
    "        self.residual_projection_upper_bounds = [[numpy.nan,] * initial_hidden_units, ] * initial_depth\n",
    "        self.residual_projection_std = [[numpy.nan,] * initial_hidden_units, ] * initial_depth\n",
    "        self.kernels = [[numpy.nan,] * initial_hidden_units, ] * initial_depth\n",
    "        self.residual_projections = [[numpy.nan,] * initial_hidden_units, ] * initial_depth\n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            for layer in self.layers: layer.bias.copy_(torch.zeros(initial_hidden_units))\n",
    "            \n",
    "        self.activation_function = torch.nn.ReLU()\n",
    "        self.output_layer_weights = torch.tensor(([1. / initial_hidden_units ** 0.5] * math.ceil(initial_hidden_units / 2.) + \n",
    "                                                  [-1. / initial_hidden_units ** 0.5] * (initial_hidden_units // 2)))\n",
    "        \n",
    "    @property        \n",
    "    def depth(self):\n",
    "        return len(self.layers) + 1\n",
    "\n",
    "    def parameters(self):\n",
    "        parameters = []\n",
    "        for layer in self.layers: parameters += list(layer.parameters())\n",
    "        return parameters\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.requires_grad_()\n",
    "        x.retain_grad()\n",
    "        self.pre_activations = []\n",
    "        self.activations = [x]\n",
    "        for layer in self.layers:\n",
    "            pre_activation = layer(x).requires_grad_()\n",
    "            pre_activation.retain_grad()\n",
    "            self.pre_activations.append(pre_activation)\n",
    "            activation = self.activation_function(pre_activation).requires_grad_()\n",
    "            activation.retain_grad()\n",
    "            self.activations.append(activation)\n",
    "            x = activation\n",
    "            \n",
    "        self.output = torch.matmul(activation, self.output_layer_weights).unsqueeze(1).requires_grad_()\n",
    "        self.output.retain_grad()\n",
    "        return self.output\n",
    "\n",
    "    def to(self, device):\n",
    "        super().to(device)\n",
    "        for layer_index, layer in enumerate(self.layers):\n",
    "            self.layers[layer_index] = layer.to(device)\n",
    "\n",
    "        self.output_layer_weights = self.output_layer_weights.to(device)\n",
    "        self.device = device\n",
    "        return self\n",
    "    \n",
    "    #def reinitialize_growth_metrics(self):\n",
    "    #    self.active_samples = [[0,] * layer.weight.shape[0] for layer in self.layers] \n",
    "    #    self.samples_not_captured = [[0,] * layer.weight.shape[0] for layer in self.layers] \n",
    "    #    self.gradients_average_norm = [[0,] * layer.weight.shape[0] for layer in self.layers] \n",
    "    #    self.average_gradient_norm = [[0,] * layer.weight.shape[0] for layer in self.layers] \n",
    "    #    self.margins = [[[]] * layer.weight.shape[0] for layer in self.layers]\n",
    "    \n",
    "    def calculate_layers_metrics(self):\n",
    "        residual = self.output.grad.squeeze() * self.output.shape[0]\n",
    "        self.residual_norm = residual.norm().item()\n",
    "        self.output.backward(torch.ones_like(self.output), retain_graph=True)\n",
    "        for layer_index, (inputs, pre_activation) in enumerate(zip(self.activations[:-1], self.pre_activations)):\n",
    "            per_sample_gradients = pre_activation.grad.unsqueeze(-1).bmm(inputs.unsqueeze(1)).permute(1, 0, 2)\n",
    "            self.gradients_average_norm[layer_index] = per_sample_gradients.norm(dim=2).mean(dim=1).tolist()\n",
    "            self.average_gradient_norm[layer_index] = per_sample_gradients.mean(dim=0).norm(dim=1).tolist()\n",
    "\n",
    "            per_sample_gradients_t = per_sample_gradients.permute(0, 2, 1)\n",
    "            kernels = per_sample_gradients.bmm(torch.pinverse(per_sample_gradients_t.bmm(per_sample_gradients)).bmm(per_sample_gradients_t))\n",
    "            self.kernels[layer_index] = kernels\n",
    "            residual_projections = torch.matmul(kernels, residual)\n",
    "            self.residual_projections[layer_index] = residual_projections\n",
    "            number_of_active_samples = (residual_projections != 0).count_nonzero(dim=1)\n",
    "            residual_projection_upper_bounds = (\n",
    "                    (1. / 2.) ** (3. / 2.) +                                # maximum variance assuming a Bernoulli distribution over align predictions.\n",
    "                    (self.noise_rate * (1. - self.noise_rate)) ** (1. / 2.)   # maximum variance assuming a the noisy samples follow a Bernoulli process.\n",
    "            ) / number_of_active_samples ** 0.5\n",
    "            residual_projection_std = torch.std(residual_projections, dim=1)\n",
    "            self.residual_projection_norm[layer_index] = residual_projections.norm(dim=1).tolist()\n",
    "            self.residual_projection_upper_bounds[layer_index] = residual_projection_upper_bounds.tolist()\n",
    "            self.residual_projection_std[layer_index] = residual_projection_std.tolist()\n",
    "\n",
    "    def remove_dead_units(self):\n",
    "        has_removed_units = False\n",
    "        for layer_index, (inputs, pre_activation) in enumerate(zip(self.activations[:-1], self.pre_activations)):\n",
    "            per_sample_gradients = pre_activation.grad.unsqueeze(-1).bmm(inputs.unsqueeze(1))\n",
    "            dead_units = (per_sample_gradients.norm(dim=0).norm(dim=1) == 0).nonzero().reshape(-1).detach().cpu().tolist()\n",
    "            if dead_units: \n",
    "                self.remove_dead_units_from_layer(dead_units, layer_index)\n",
    "                has_removed_units = True\n",
    "            \n",
    "        return has_removed_units\n",
    "    \n",
    "    def remove_dead_units_from_layer(self, dead_units, layer_index):\n",
    "        input_dimension = self.input_dimension if layer_index == 0 else self.layers[layer_index - 1].weight.shape[0]\n",
    "        number_of_units = self.layers[layer_index].weight.shape[0]\n",
    "        if len(dead_units) == number_of_units: \n",
    "            self.layers[layer_index] = torch.nn.Linear(\n",
    "                input_dimension if layer_index == 0 else self.initial_hidden_units, self.initial_hidden_units, \n",
    "                bias=self.bias\n",
    "            )\n",
    "            for next_layers in range(layer_index + 1, len(self.layers)): \n",
    "                del self.layers[next_layers]\n",
    "\n",
    "            self.output_layer_weights = torch.tensor(\n",
    "                [1. / self.initial_hidden_units ** 0.5,] * math.ceil(self.initial_hidden_units / 2.) + \n",
    "                [-1. / self.initial_hidden_units ** 0.5,] * (self.initial_hidden_units // 2)\n",
    "            )\n",
    "            print(f'All {len(dead_units)} units at layer {layer_index} are dead. The network is reinitialized from layer {layer_index} to be of depth {self.depth}')\n",
    "\n",
    "        else:\n",
    "            alive_units = [unit_index for unit_index in range(number_of_units) if unit_index not in dead_units]\n",
    "            alive_units_weights = self.layers[layer_index].weight.data[alive_units]\n",
    "            alive_units_biases = self.layers[layer_index].bias.data[alive_units]\n",
    "            self.layers[layer_index] = torch.nn.Linear(input_dimension, len(alive_units), bias=self.bias).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                self.layers[layer_index].weight.copy_(alive_units_weights)\n",
    "                self.layers[layer_index].bias.copy_(alive_units_biases)\n",
    "\n",
    "            if layer_index == len(self.layers) - 1: \n",
    "                self.output_layer_weights = self.output_layer_weights[alive_units].clone()\n",
    "\n",
    "            else:\n",
    "                next_layer_units = self.layers[layer_index + 1].weight.shape[0]\n",
    "                next_layer_alive_units_weights = self.layers[layer_index + 1].weight[:, alive_units].clone()\n",
    "                next_layer_alive_units_biases = self.layers[layer_index + 1].bias.data.clone()\n",
    "                self.layers[layer_index + 1] = torch.nn.Linear(len(alive_units), next_layer_units, bias=self.bias).to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    self.layers[layer_index + 1].weight.copy_(next_layer_alive_units_weights)\n",
    "                    self.layers[layer_index + 1].bias.copy_(next_layer_alive_units_biases)\n",
    "\n",
    "            print(f'{len(dead_units)} units at layer {layer_index} are dead and were removed')\n",
    "    \n",
    "    def grow(self):\n",
    "        for layer_index in range(self.depth - 1):\n",
    "            samples_not_captured = (self.residual_projections[layer_index].norm(dim=0) == 0).nonzero().squeeze().detach().cpu().tolist()\n",
    "            units_with_variance_beyond_worst_case = (\n",
    "                torch.tensor(self.residual_projection_upper_bounds[layer_index]) < torch.tensor(self.residual_projection_std[layer_index])\n",
    "            ).count_nonzero()\n",
    "                \n",
    "            if samples_not_captured:\n",
    "                self.grow_width(layer_index, samples_not_captured)\n",
    "                return True\n",
    "            \n",
    "            if units_with_variance_beyond_worst_case:\n",
    "                self.grow_depth(layer_index)\n",
    "                return True\n",
    "\n",
    "            return False\n",
    "                    \n",
    "    def grow_width(self, layer_index, samples_not_captured):\n",
    "        former_weights = self.layers[layer_index].weight.data.clone()\n",
    "        former_biases = self.layers[layer_index].bias.data.clone()\n",
    "        input_dimension = former_weights.shape[1]\n",
    "        hidden_units = former_weights.shape[0]\n",
    "        self.layers[layer_index] = torch.nn.Linear(input_dimension, hidden_units + 2, bias=self.bias).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            self.layers[layer_index].weight[:-2].copy_(former_weights)\n",
    "            self.layers[layer_index].bias[:-2].copy_(former_biases)\n",
    "            self.layers[layer_index].weight[-2:].copy_(self.activations[layer_index][samples_not_captured].mean(dim=0).unsqueeze(0).repeat(2, 1))\n",
    "            self.layers[layer_index].bias[-2:].copy_(torch.tensor([0.,] * 2).to(self.device))\n",
    "\n",
    "        if layer_index == self.depth - 2:    \n",
    "            next_layer_new_units_weights = torch.tensor([-1., 1.]).to(self.device) / (hidden_units + 2) ** 0.5\n",
    "            self.output_layer_weights = torch.cat([self.output_layer_weights, next_layer_new_units_weights]).sign()\n",
    "        \n",
    "        else:\n",
    "            next_layer_weights = self.layers[layer_index + 1].weight.data.clone()\n",
    "            next_layer_biases = self.layers[layer_index + 1].bias.data.clone()\n",
    "            next_layer_hidden_units = self.layers[layer_index + 1].weight.shape[0]\n",
    "            next_layer_influence_in_output = self.pre_activations[layer_index + 1].grad[samples_not_captured].mean(dim=0).sign()\n",
    "            next_layer_influence_in_output[next_layer_influence_in_output==0.] = 1.\n",
    "            next_layer_new_units_weights = torch.tensor([[-1., 1.],] * next_layer_hidden_units).to(self.device) / (hidden_units + 2) ** 0.5\n",
    "            next_layer_new_weights = torch.cat([next_layer_weights, next_layer_new_units_weights], dim=1)\n",
    "            self.layers[layer_index + 1] = torch.nn.Linear(hidden_units + 2, next_layer_hidden_units, bias=self.bias).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                self.layers[layer_index + 1].weight.copy_(next_layer_new_weights)\n",
    "                self.layers[layer_index + 1].bias.copy_(next_layer_biases)\n",
    "        \n",
    "        print(f'Width growth: Two unit with opposing signs were added to layer {layer_index} which now has {hidden_units + 2} units')\n",
    "\n",
    "    def grow_depth(self, layer_index):\n",
    "        hidden_units = self.layers[layer_index].weight.shape[0]\n",
    "        self.layers.insert(layer_index + 1, torch.nn.Linear(hidden_units, hidden_units, self.bias).to(self.device))\n",
    "        with torch.no_grad():\n",
    "            self.layers[layer_index + 1].weight.copy_(torch.eye(hidden_units).to(self.device))\n",
    "            self.layers[layer_index + 1].bias.copy_(torch.zeros(hidden_units).to(self.device))\n",
    "\n",
    "        for list in [self.gradients_average_norm, self.average_gradient_norm, self.residual_projection_norm, \n",
    "                     self.residual_projection_upper_bounds, self.residual_projection_std, self.kernels, self.residual_projections]:\n",
    "            list.append([[numpy.nan,] * hidden_units])\n",
    "            \n",
    "        print(f'Depth growth: A ReLU layer with identity weights was inserted at depth {layer_index + 1}. Total depth including output layer is {self.depth}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1b5ad17-55b7-4878-9be1-bdd85a8e02cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipycanvas\n",
    "\n",
    "training_canvas, gradients_norms_canvas, input_domain_canvas = ipycanvas.Canvas(), ipycanvas.Canvas(), ipycanvas.Canvas()\n",
    "training_canvas.width = training_canvas.height = 800\n",
    "gradients_norms_canvas.width = 1200; gradients_norms_canvas.height = 400\n",
    "input_domain_canvas.width = input_domain_canvas.height = 800\n",
    "training_canvas.font = gradients_norms_canvas.font = input_domain_canvas.font = \"30px arial\"\n",
    "args = ('Results will appear as processed', training_canvas.width / 4, training_canvas.height / 3)\n",
    "training_canvas.fill_text(*args); gradients_norms_canvas.fill_text(*args); input_domain_canvas.fill_text(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da6eb3ab-e384-4f3b-8a58-bdd94aed2448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a47914b6b07496a973daed286ff1dea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(height=800, width=800)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b77d780b",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m train_loss_value \u001b[38;5;241m=\u001b[39m test(train_data, model, test_loss, device, \n\u001b[1;32m     26\u001b[0m                         calculate_gradients\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m%\u001b[39m plots_epochs_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m experiment[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(run[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m run[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;241m<\u001b[39m experiment[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconvergence_epsilon\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m---> 29\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_layers_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mremove_dead_units():\n\u001b[1;32m     31\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "Cell \u001b[0;32mIn[2], line 137\u001b[0m, in \u001b[0;36mGrowingNeuralNet.calculate_layers_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage_gradient_norm[layer_index] \u001b[38;5;241m=\u001b[39m per_sample_gradients\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    136\u001b[0m per_sample_gradients_t \u001b[38;5;241m=\u001b[39m per_sample_gradients\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 137\u001b[0m kernels \u001b[38;5;241m=\u001b[39m \u001b[43mper_sample_gradients\u001b[49m\u001b[38;5;241m.\u001b[39mbmm(torch\u001b[38;5;241m.\u001b[39mpinverse(per_sample_gradients_t\u001b[38;5;241m.\u001b[39mbmm(per_sample_gradients))\u001b[38;5;241m.\u001b[39mbmm(per_sample_gradients_t))\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernels[layer_index] \u001b[38;5;241m=\u001b[39m kernels\n\u001b[1;32m    139\u001b[0m residual_projections \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(kernels, residual)\n",
      "Cell \u001b[0;32mIn[2], line 137\u001b[0m, in \u001b[0;36mGrowingNeuralNet.calculate_layers_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage_gradient_norm[layer_index] \u001b[38;5;241m=\u001b[39m per_sample_gradients\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    136\u001b[0m per_sample_gradients_t \u001b[38;5;241m=\u001b[39m per_sample_gradients\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 137\u001b[0m kernels \u001b[38;5;241m=\u001b[39m \u001b[43mper_sample_gradients\u001b[49m\u001b[38;5;241m.\u001b[39mbmm(torch\u001b[38;5;241m.\u001b[39mpinverse(per_sample_gradients_t\u001b[38;5;241m.\u001b[39mbmm(per_sample_gradients))\u001b[38;5;241m.\u001b[39mbmm(per_sample_gradients_t))\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernels[layer_index] \u001b[38;5;241m=\u001b[39m kernels\n\u001b[1;32m    139\u001b[0m residual_projections \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(kernels, residual)\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.virtualenvs/nns_growth/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/nns_growth/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from plots import plot_experiment\n",
    "\n",
    "plots_epochs_interval = 2\n",
    "\n",
    "device = initialize(experiment['seed'])\n",
    "model = GrowingNeuralNet(**experiment).to(device)\n",
    "train_loss = test_loss = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=experiment['learning_rate'])\n",
    "run = {\n",
    "    'distinction': '1',\n",
    "    'train': [test(train_data, model, test_loss, device)],\n",
    "    'train_time': [0],\n",
    "    'test': [test(test_data, model, test_loss, device, verbose=False)]\n",
    "}\n",
    "experiment['models_runs'].append(run)\n",
    "summary_metrics = ('gradients_average_norm', 'average_gradient_norm', 'residual_projection_norm', \n",
    "                   'residual_projection_upper_bounds', 'residual_projection_std', 'residual_norm')\n",
    "for field in summary_metrics: run[field] = []\n",
    "\n",
    "for epoch in range(1, experiment['epochs'] + 1):\n",
    "    start_time = time.time()\n",
    "    train(train_data, model, train_loss, optimizer, device, verbose=False)\n",
    "    end_time = time.time()\n",
    "    train_time = run['train_time'][-1] + end_time - start_time\n",
    "    train_loss_value = test(train_data, model, test_loss, device, \n",
    "                            calculate_gradients=True, retain_graph=True, verbose=False)\n",
    "    \n",
    "    if (epoch % plots_epochs_interval == 0 or epoch == experiment['epochs']) and abs(run['train'][-1] - run['train'][-2]) < experiment['convergence_epsilon']:\n",
    "        model.calculate_layers_metrics()\n",
    "        if model.remove_dead_units():\n",
    "            optimizer.zero_grad()\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=experiment['learning_rate'])\n",
    "        else:\n",
    "            print(f'Convergence achieve according to convergence_epsilon = {experiment[\"convergence_epsilon\"]}')\n",
    "            if model.grow():\n",
    "                optimizer.zero_grad()\n",
    "                optimizer = torch.optim.SGD(model.parameters(), lr=experiment['learning_rate'])\n",
    "\n",
    "        for field in summary_metrics: run[field].append(model.__getattribute__(field))\n",
    "\n",
    "    test_loss_value = test(test_data, model, test_loss, device, verbose=False)\n",
    "    run['train'].append(train_loss_value)\n",
    "    run['train_time'].append(train_time)\n",
    "    run['test'].append(test_loss_value)\n",
    "    if epoch % plots_epochs_interval == 0 or epoch == experiment['epochs']:\n",
    "        plot_experiment(experiment, training_canvas)\n",
    "        #plot_gradients_norms(run, canvas=gradients_norms_canvas, summary_frequency=plots_epochs_interval)\n",
    "        #plot_samples_and_model_activation_and_neurons_hyperplanes(\n",
    "        #    dataloader=train_data, model=model, rotation_matrix=rotation_matrix, \n",
    "        #    canvas=input_domain_canvas, **experiment\n",
    "        #)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "582da670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan]], device='mps:0', grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.residual_projections[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
