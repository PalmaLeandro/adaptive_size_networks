{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb264f59-d0b4-4a3a-bab6-2754b989d5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, numpy, torch, matplotlib.pyplot\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))  # Allow repository modules to be imported"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f41de7-34b3-4fec-bf4c-295f7e8b45ff",
   "metadata": {},
   "source": [
    "# Concentric spheres data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8546fab-ad68-4215-bfc4-2e954ae8785d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msettings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconcentric_spheres\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_dataloader\n\u001b[1;32m      3\u001b[0m data_setting \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1234\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_dimension\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m5000\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmargin\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.3\u001b[39m\n\u001b[1;32m     10\u001b[0m }\n\u001b[0;32m---> 12\u001b[0m data, rotation_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata_setting\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m _, (inputs, labels) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28menumerate\u001b[39m(data))\n\u001b[1;32m     15\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m matplotlib\u001b[38;5;241m.\u001b[39mpyplot\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n",
      "File \u001b[0;32m~/Documents/git/nns_growth/notebooks/settings/concentric_spheres.py:31\u001b[0m, in \u001b[0;36mget_dataloader\u001b[0;34m(sample_size, batch_size, rotation_matrix, *args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dataloader\u001b[39m(sample_size:\u001b[38;5;28mint\u001b[39m, batch_size:\u001b[38;5;28mint\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, rotation_matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rotation_matrix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m         dataset, rotation_matrix_ \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m         dataset \u001b[38;5;241m=\u001b[39m get_dataset(\u001b[38;5;241m*\u001b[39margs, sample_size\u001b[38;5;241m=\u001b[39msample_size, rotation_matrix\u001b[38;5;241m=\u001b[39mrotation_matrix, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)    \n",
      "File \u001b[0;32m~/Documents/git/nns_growth/notebooks/settings/concentric_spheres.py:24\u001b[0m, in \u001b[0;36mget_dataset\u001b[0;34m(input_dimension, sample_size, spheres_dimension, number_of_spheres, margin, mean, rotation_matrix, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_dimensions:\n\u001b[1;32m     22\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([inputs, torch\u001b[38;5;241m.\u001b[39mnormal(mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, std\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, size\u001b[38;5;241m=\u001b[39m(sample_size, extra_dimensions), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m rotation_matrix_ \u001b[38;5;241m=\u001b[39m \u001b[43mscipy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspecial_ortho_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrvs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dimension\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m rotation_matrix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m rotation_matrix\n\u001b[1;32m     25\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mmm(torch\u001b[38;5;241m.\u001b[39mtensor(rotation_matrix_, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[1;32m     26\u001b[0m dataset \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mTensorDataset(inputs, labels)\n",
      "File \u001b[0;32m~/.virtualenvs/nns_growth/lib/python3.10/site-packages/scipy/stats/_multivariate.py:3681\u001b[0m, in \u001b[0;36mspecial_ortho_group_gen.rvs\u001b[0;34m(self, dim, size, random_state)\u001b[0m\n\u001b[1;32m   3677\u001b[0m     x \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt((norm2 \u001b[38;5;241m-\u001b[39m x0\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.\u001b[39m)[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m   3679\u001b[0m     \u001b[38;5;66;03m# Householder transformation, without vectorization the RHS can be\u001b[39;00m\n\u001b[1;32m   3680\u001b[0m     \u001b[38;5;66;03m# written as outer(H @ x, x) (apart from the slicing)\u001b[39;00m\n\u001b[0;32m-> 3681\u001b[0m     H[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :, n:] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmatmul(H[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :, n:], xcol) \u001b[38;5;241m*\u001b[39m xrow\n\u001b[1;32m   3683\u001b[0m D[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(dim\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mD[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mprod(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   3685\u001b[0m \u001b[38;5;66;03m# Without vectorization this could be written as H = diag(D) @ H,\u001b[39;00m\n\u001b[1;32m   3686\u001b[0m \u001b[38;5;66;03m# left-multiplication by a diagonal matrix amounts to multiplying each\u001b[39;00m\n\u001b[1;32m   3687\u001b[0m \u001b[38;5;66;03m# row of H by an element of the diagonal, so we add a dummy axis for\u001b[39;00m\n\u001b[1;32m   3688\u001b[0m \u001b[38;5;66;03m# the column index\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from settings.concentric_spheres import get_dataloader\n",
    "\n",
    "data_setting = {\n",
    "    'seed': 1234,\n",
    "    'input_dimension': 200, \n",
    "    'spheres_dimension': 2, \n",
    "    'number_of_spheres': 4, \n",
    "    'sample_size': 50, \n",
    "    'margin': 0.3\n",
    "}\n",
    "\n",
    "data, rotation_matrix = get_dataloader(**data_setting)\n",
    "_, (inputs, labels) = next(enumerate(data))\n",
    "\n",
    "fig, ax = matplotlib.pyplot.subplots(figsize=(10, 10))\n",
    "inputs_ = numpy.matmul(inputs.detach().cpu().numpy(), rotation_matrix.transpose())\n",
    "ax.scatter(inputs_[:, 0], inputs_[:, 1], c=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af03851b-250c-4b33-95df-681aec122c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_setting = {\n",
    "    'epochs': 10000,\n",
    "    'learning_rate': 0.05,\n",
    "    'batch_size': data_setting['sample_size'],\n",
    "    'hidden_units': 2,\n",
    "    'bias': True,\n",
    "    'initialization_variance': 1. / data_setting['input_dimension'] ** 1.2\n",
    "}\n",
    "\n",
    "class TwoLayerNeuralNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dimension:int, hidden_units:int, initialization_variance:float=1., bias:bool=True, *args, **kwargs):\n",
    "        super(TwoLayerNeuralNet, self).__init__()\n",
    "        self.device = 'cpu'\n",
    "        self.hidden_units = hidden_units\n",
    "        self.input_dimension = input_dimension\n",
    "        if hidden_units % 2 == 1:\n",
    "            hidden_units -= 1\n",
    "            print(f'Only even number of hidden units allowed. Switching to hidden units = {hidden_units}') \n",
    "\n",
    "        self.input_layer = torch.nn.Linear(input_dimension, hidden_units, bias=bias)\n",
    "        #torch.nn.init.normal_(self.input_layer.weight, std=initialization_variance ** 0.5)\n",
    "        #if bias: self.input_layer.bias.data = torch.zeros(input_dimension, requires_grad=True)\n",
    "\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        output_layer_weights = [1. / hidden_units ** 0.5] * (hidden_units // 2) + [-1. / hidden_units ** 0.5] * (hidden_units // 2)\n",
    "        self.output_layer_weights = torch.tensor(output_layer_weights)\n",
    "\n",
    "        self.dummy_variable1 = torch.zeros(hidden_units, hidden_units, requires_grad=True)\n",
    "        self.dummy_variable1.retain_grad()\n",
    "\n",
    "        self.dummy_variable2 = torch.zeros(input_dimension, input_dimension, requires_grad=True)\n",
    "        self.dummy_variable2.retain_grad()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #dummy_term2 = (\n",
    "        #    x.reshape(-1, self.input_dimension, 1).bmm(x.mm(self.dummy_variable2).reshape(-1, 1, self.input_dimension))\n",
    "        #)\n",
    "        self.pre_activations = self.input_layer(x).requires_grad_() #+ dummy_term2\n",
    "        self.pre_activations.retain_grad()\n",
    "        self.activations = self.activation(self.pre_activations).requires_grad_()\n",
    "        self.activations.retain_grad()\n",
    "        #dummy_term1 = (\n",
    "        #    self.activations.unsqueeze(1).bmm(self.activations.mm(self.dummy_variable1)\n",
    "        #        .reshape(-1, self.hidden_units, 1)).reshape(-1)\n",
    "        #)\n",
    "        output = torch.matmul(self.activations, self.output_layer_weights) #+ dummy_term1\n",
    "        return output.unsqueeze(1)\n",
    "\n",
    "    def to(self, device):\n",
    "        super().to(device)\n",
    "        self.output_layer_weights = self.output_layer_weights.to(device)\n",
    "        self.dummy_variable1 = self.dummy_variable1.to(device)\n",
    "        self.dummy_variable1.retain_grad()\n",
    "        self.dummy_variable2 = self.dummy_variable2.to(device)\n",
    "        self.dummy_variable2.retain_grad()\n",
    "        self.device = device\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b5ad17-55b7-4878-9be1-bdd85a8e02cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipycanvas\n",
    "\n",
    "training_canvas, gradients_norms_canvas, input_domain_canvas = ipycanvas.Canvas(), ipycanvas.Canvas(), ipycanvas.Canvas()\n",
    "training_canvas.width = training_canvas.height = 800\n",
    "gradients_norms_canvas.width = 1200; gradients_norms_canvas.height = 600\n",
    "input_domain_canvas.width = input_domain_canvas.height = 800\n",
    "training_canvas.font = gradients_norms_canvas.font = input_domain_canvas.font = \"30px arial\"\n",
    "args = ('Results will appear as processed', training_canvas.width / 4, training_canvas.height / 3)\n",
    "training_canvas.fill_text(*args); gradients_norms_canvas.fill_text(*args); input_domain_canvas.fill_text(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6eb3ab-e384-4f3b-8a58-bdd94aed2448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121cbc4100d24a5c85a99000ca036f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(height=800, width=800)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6177433a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d223d87823024436b93375cc10adf396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(height=600, width=1200)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients_norms_canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ee36ad-bca0-4205-840f-6842e661017a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54bb641726b14e308b0823d3e219794b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(height=800, width=800)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_domain_canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3189a0f4-e94c-4271-aea1-a28bd336434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.optimization import initialize, train, test, Accuracy\n",
    "from plots import plot_experiment, plot_gradients_norms, plot_samples_and_model_activation_and_neurons_hyperplanes\n",
    "\n",
    "plots_epochs_interval = 100\n",
    "\n",
    "experiment = {\n",
    "    **data_setting,\n",
    "    **model_setting,\n",
    "    'train': 'Accuracy',\n",
    "    'test': 'Accuracy',\n",
    "    'train_time': 'seconds',\n",
    "    'models_runs': []\n",
    "}\n",
    "\n",
    "device = initialize(experiment['seed'])\n",
    "train_data, rotation_matrix = get_dataloader(**experiment) \n",
    "test_data = get_dataloader(**experiment, rotation_matrix=rotation_matrix)\n",
    "model = TwoLayerNeuralNet(**experiment).to(device)\n",
    "train_loss, test_loss = torch.nn.BCEWithLogitsLoss(), Accuracy\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=experiment['learning_rate'])\n",
    "run = {\n",
    "    'distinction': '1',\n",
    "    'train': [test(train_data, model, test_loss, device)],\n",
    "    'train_time': [0],\n",
    "    'test': [test(test_data, model, test_loss, device, verbose=False)]\n",
    "}\n",
    "experiment['models_runs'].append(run)\n",
    "\n",
    "def gradients_summary(model, *args, epoch_frequency=1):\n",
    "    epoch = len(run.get('train', []))\n",
    "    if epoch == 0 or epoch % epoch_frequency != 0: return\n",
    "\n",
    "    sample_size = experiment['sample_size']\n",
    "    pre_activations_gradients_average_norm = model.pre_activations.grad.mean(dim=0).norm().item() * sample_size\n",
    "    run['pre_activations_gradients_average_norm'] = \\\n",
    "        run.get('pre_activations_gradients_average_norm', []) + [pre_activations_gradients_average_norm]\n",
    "    \n",
    "    activations_gradients_average_norm = model.activations.grad.mean(dim=0).norm().item() * sample_size\n",
    "    run['activations_gradients_average_norm'] = \\\n",
    "        run.get('activations_gradients_average_norm', []) + [activations_gradients_average_norm]\n",
    "\n",
    "    pre_activations_average_gradient_norm = model.pre_activations.grad.norm(dim=1).mean().item() * sample_size\n",
    "    run['pre_activations_average_gradient_norm'] = \\\n",
    "        run.get('pre_activations_average_gradient_norm', []) + [pre_activations_average_gradient_norm]\n",
    "    \n",
    "    activations_average_gradient_norm = model.activations.grad.norm(dim=1).mean().item() * sample_size\n",
    "    run['activations_average_gradient_norm'] = \\\n",
    "        run.get('activations_average_gradient_norm', []) + [activations_average_gradient_norm]\n",
    "\n",
    "plot_samples_and_model_activation_and_neurons_hyperplanes(dataloader=train_data, model=model, rotation_matrix=rotation_matrix, \n",
    "                                                          **experiment, canvas=input_domain_canvas)\n",
    "\n",
    "for epoch in range(1, experiment['epochs'] + 1):\n",
    "    start_time = time.time()\n",
    "    train(train_data, model, train_loss, optimizer, device, verbose=False, callback=gradients_summary)\n",
    "    end_time = time.time()\n",
    "    train_time = run['train_time'][-1] + end_time - start_time\n",
    "    train_loss_value = test(train_data, model, test_loss, device, verbose=False)\n",
    "    test_loss_value = test(test_data, model, test_loss, device, verbose=False)\n",
    "    run['train'].append(train_loss_value)\n",
    "    run['train_time'].append(train_time)\n",
    "    run['test'].append(test_loss_value)\n",
    "    if epoch % plots_epochs_interval == 0 or epoch == experiment['epochs']:\n",
    "        plot_experiment(experiment, training_canvas)\n",
    "        plot_gradients_norms(run, canvas=gradients_norms_canvas)\n",
    "        plot_samples_and_model_activation_and_neurons_hyperplanes(\n",
    "            dataloader=train_data, model=model, rotation_matrix=rotation_matrix, **experiment, \n",
    "            canvas=input_domain_canvas\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b4197d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8421f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
