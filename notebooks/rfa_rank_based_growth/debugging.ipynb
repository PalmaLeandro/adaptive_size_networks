{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce9b5d69",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# RFA-based NN growth experiment (WIP)\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6f1591-fe01-4687-a72b-b27e07d48b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "input_dimension = 2\n",
    "hidden_units = 3\n",
    "\n",
    "layers = [\n",
    "    [[1, 0, -1],\n",
    "     [0, 1, 0]],\n",
    "    [1, -1, 1]\n",
    "]\n",
    "\n",
    "dummy_variable = torch.zeros(hidden_units, hidden_units, requires_grad=True)\n",
    "dummy_variable.retain_grad()\n",
    "\n",
    "\n",
    "x = torch.tensor([[1., 0.], [0., 1.], [-1., 0.]])\n",
    "y = torch.tensor([1., 0., 1.])\n",
    "\n",
    "input_layer = torch.Tensor(layers[0])\n",
    "output_layer = torch.Tensor(layers[1])\n",
    "\n",
    "\n",
    "activations = torch.nn.ReLU()(torch.matmul(x, input_layer))\n",
    "dummy_term = activations.unsqueeze(1).bmm(activations.mm(dummy_variable).reshape(-1, hidden_units, 1)).reshape(-1)\n",
    "\n",
    "pred = torch.matmul(activations, torch.Tensor(layers[1])) + dummy_term\n",
    "pred.retain_grad()\n",
    "\n",
    "import numpy, torch\n",
    "\n",
    "from experiment import TwoLayerNeuralNet\n",
    "from settings.noisy_xor import get_dataloader\n",
    "from utils.optimization import Accuracy, initialize\n",
    "\n",
    "initialize(experiment_results['seed'])\n",
    "\n",
    "data, rotation_matrix = get_dataloader(**experiment_results)\n",
    "_, (inputs, labels) = next(enumerate(data))\n",
    "\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "loss = loss_fn(pred, y)\n",
    "\n",
    "loss.backward()\n",
    "dummy_variable.grad\n",
    "activations.reshape(-1, hidden_units, 1).bmm(activations.reshape(-1, 1, hidden_units))\n",
    "numpy.linalg.norm((dummy_variable.grad / max_eigenvalue).detach().cpu().numpy()) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37d96eba-ed70-4464-a5ca-ce59bc43ecb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48c11332-594b-40d1-9968-4800e2c5471c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [4, 5]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[..., :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c050abf3-1d75-4ebb-bc25-95a1b0c2ac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment import TwoLayerNeuralNet\n",
    "from settings.noisy_xor import get_dataloader\n",
    "from utils.optimization import Accuracy, initialize\n",
    "\n",
    "initialize(experiment_results['seed'])\n",
    "\n",
    "data, rotation_matrix = get_dataloader(**experiment_results)\n",
    "_, (inputs, labels) = next(enumerate(data))\n",
    "\n",
    "model = TwoLayerNeuralNet.load('./models/', {**experiment_results, 'run': 0})\n",
    "\n",
    "print(f'Accuracy: {Accuracy(model(inputs), labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcbef48-7e31-4415-89df-9a49ed625607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, torch\n",
    "\n",
    "input_dimension = experiment_results['input_dimension']\n",
    "x = numpy.array([[1., 0.], [0., 1.], [-1., 0.], [0., -1.], [1., -1.]])\n",
    "x = numpy.concatenate([x, numpy.repeat(numpy.repeat(0., input_dimension - 2)[numpy.newaxis, :], len(x), axis=0)], axis=1)\n",
    "x = numpy.matmul(x, rotation_matrix)\n",
    "x = torch.tensor(x, dtype=torch.float)\n",
    "y = torch.tensor([0., 1., 0., 1., 1.]).unsqueeze(1)\n",
    "\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "pred = model(x)\n",
    "loss = loss_fn(pred, y)\n",
    "#loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7d7402-4c17-4a81-87a1-62a303eebb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot\n",
    "\n",
    "histogram_resolution = 20\n",
    "\n",
    "def histogram_bars(histogram_frequencies, histogram_bins):\n",
    "    histogram_bins = histogram_bins.detach().cpu().numpy()[:-1]\n",
    "    histogram_bins_pace = min(histogram_bins[1] - histogram_bins[0], 0.01)\n",
    "    histogram_bins += histogram_bins_pace / 2.\n",
    "    histogram_frequencies = histogram_frequencies.detach().cpu().numpy()\n",
    "    return histogram_bins, histogram_frequencies / histogram_frequencies.sum(), histogram_bins_pace\n",
    "\n",
    "fig, (activations_ax, loss_ax, scatter_ax) = matplotlib.pyplot.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "activations_ax.set_title('Activations')\n",
    "activations_ax.set_xlabel('activation L2 norm')\n",
    "activations_ax.set_ylabel('% samples')\n",
    "\n",
    "loss_ax.set_title('Loss')\n",
    "loss_ax.set_xlabel('loss norm')\n",
    "loss_ax.set_ylabel('% samples')\n",
    "\n",
    "scatter_ax.set_title('Activations vs Loss')\n",
    "scatter_ax.set_xlabel('activation L2 norm')\n",
    "scatter_ax.set_ylabel('loss')\n",
    "\n",
    "for cluster_index, cluster in zip(inputs_cluster.unique().tolist(), [[1., 0.], [0., 1.], [-1., 0.], [0., -1.]]):\n",
    "    activations_ax.bar(*histogram_bars(*torch.histogram(data[inputs_cluster==cluster_index][:, 0], histogram_resolution)), label=f'cluster {cluster}', alpha=0.3)\n",
    "    loss_ax.bar(*histogram_bars(*torch.histogram(data[inputs_cluster==cluster_index][:, 1], histogram_resolution)), label=f'cluster {cluster}', alpha=0.3)\n",
    "    scatter_ax.scatter(data[inputs_cluster==cluster_index][:, 0].detach().numpy(), data[inputs_cluster==cluster_index][:, 1].detach().numpy(), label=f'cluster {cluster}')\n",
    "\n",
    "for ax in (activations_ax, loss_ax, scatter_ax): ax.legend()\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0792bec2-f46f-443c-b155-ee0a1735aeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "input_dimension = 2\n",
    "hidden_units = 2\n",
    "\n",
    "class TwoLayerNeuralNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(TwoLayerNeuralNet, self).__init__()\n",
    "        self.input_layer = torch.nn.Parameter(torch.tensor([[1., 0.],\n",
    "                                                            [0., 1.]], requires_grad=True))\n",
    "        self.activation_fn = torch.nn.ReLU()\n",
    "        self.output_layer = torch.tensor([-1., 1.])\n",
    "        self.dummy_variable = torch.zeros(hidden_units, hidden_units, requires_grad=True)\n",
    "        self.dummy_variable.retain_grad()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.pre_activations = x.mm(self.input_layer).requires_grad_()\n",
    "        self.pre_activations.retain_grad()\n",
    "        \n",
    "        self.activations = self.activation_fn(self.pre_activations).requires_grad_()\n",
    "        self.activations.retain_grad()\n",
    "        \n",
    "        dummy_term = self.activations.unsqueeze(1).bmm(self.activations.mm(self.dummy_variable).reshape(-1, hidden_units, 1)).reshape(-1)\n",
    "        \n",
    "        self.pred = torch.matmul(self.activations, self.output_layer) + dummy_term\n",
    "        self.pred.retain_grad()\n",
    "        return self.pred\n",
    "\n",
    "model = TwoLayerNeuralNet()\n",
    "initial_input_layer = model.input_layer.data.detach().cpu().numpy().tolist()\n",
    "initial_output_layer = model.output_layer.data.detach().cpu().numpy().tolist()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1.)\n",
    "\n",
    "x = torch.tensor([[1., 0.], [0., 1.], [-1., 0.], [0., -1.], [-1., -1.]])\n",
    "y = torch.tensor([0., 1., 0., 1., 1.])\n",
    "\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "pred = model(x)\n",
    "loss = loss_fn(pred, y)\n",
    "\n",
    "#optimizer.zero_grad()\n",
    "#loss.backward()\n",
    "#optimizer.step()\n",
    "\n",
    "final_input_layer = model.input_layer.data.detach().cpu().numpy().tolist()\n",
    "final_output_layer = model.output_layer.data.detach().cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "85918a03-da1c-48cd-a93c-deb42e4a5199",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.BCEWithLogitsLoss(reduction='none')(torch.Tensor([0., 0.]), torch.Tensor([0., 1.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ae825d8e-1919-434a-ad86-3928eab0e6e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6931, 0.6931])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0b910eb4-a227-4639-8a53-0e8d8a4b1b14",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 2 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: a Tensor with 2 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "32563244-6572-4971-a9bd-0a78945a9db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'none'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn.reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "67c78017-c6f7-419d-ab2f-7956d5cbbef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3133, 0.3133, 0.6931, 0.6931, 0.6931],\n",
       "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "80308ae2-2937-4f8d-ad83-e38cd339af90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5412, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "774e8c43-4160-4784-a775-6cc1f3336fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 0.0], [0.0, 1.0]]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_input_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d0b03ec9-7f6f-4b94-a03c-7cc312b616ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 0.0], [0.0, 1.0]]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_input_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5ce88bcc-0c95-4461-9396-32f34cf52267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0.])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.pre_activations.grad.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "545d931e-449f-4f96-a0d4-ad598d2e9129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.pre_activations.grad.sum(axis=0).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9faf981e-1ddc-4381-932f-be2cfb68a106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5000,  0.5000])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.activations.grad.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a812499e-d59d-43f0-9100-0d0c710f864d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7071)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.activations.grad.sum(axis=0).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f508946b-1f0a-4c18-8d35-06c4d7f3a1c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5000])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.pred.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00a17d4-888f-41a4-ad97-1ac89c0a7ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dummy_variable.grad\n",
    "activations.reshape(-1, hidden_units, 1).bmm(activations.reshape(-1, 1, hidden_units))\n",
    "numpy.linalg.norm((dummy_variable.grad / max_eigenvalue).detach().cpu().numpy()) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1bd674b0-3b4c-4975-92b5-afb2d4bf05d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tw/j68gkt955z57h83crjdjjrg80000gr/T/ipykernel_67226/2536819858.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  torch.nn.functional.softmax(pred)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.3333, 0.3333, 0.3333], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.softmax(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd69a577-b04b-4652-b7a7-64bc346fa89a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09830596662074093"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + math.exp(-x))\n",
    "\n",
    "sigmoid(1) * (1-sigmoid(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nns_growth",
   "language": "python",
   "name": "nns_growth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
